"""  
RAGå¤„ç†å™¨æ¨¡å—
è´Ÿè´£æ–‡æ¡£å‘é‡åŒ–ã€æ£€ç´¢å’Œä¸Šä¸‹æ–‡æ„å»º
æ”¯æŒå¤šæ¨¡æ€å†…å®¹ï¼ˆæ–‡æœ¬å’Œå›¾ç‰‡ï¼‰
"""

import os
import asyncio
import json
from typing import List, Dict, Optional, Tuple, Union
import openai
import tiktoken
import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
import hashlib
import time
import discord
from discord.ext import commands
from cogs.rag_indexer import RAGIndexer
from cogs.multimodal_embedding import (
    MultimodalEmbeddingHandler,
    MultimodalDocument,
    ContentType,
    load_image_as_bytes
)


class RAGProcessor:
    """RAGå¤„ç†å™¨ä¸»ç±»"""
    
    def __init__(self, db_path: str = "./rag_data/chroma_db"):
        """
        åˆå§‹åŒ–RAGå¤„ç†å™¨
        
        Args:
            db_path: ChromaDBæ•°æ®åº“è·¯å¾„
        """
        self.db_path = db_path
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.chunk_size = int(os.getenv("RAG_CHUNK_SIZE", "500"))  # ä»ç¯å¢ƒå˜é‡è¯»å–åˆ†å—å¤§å°ï¼ˆtokensï¼‰
        self.chunk_overlap = int(os.getenv("RAG_CHUNK_OVERLAP", "50"))  # ä»ç¯å¢ƒå˜é‡è¯»å–é‡å å¤§å°ï¼ˆtokensï¼‰
        self.top_k = int(os.getenv("RAG_TOP_K", "5"))
        self.min_similarity = float(os.getenv("RAG_MIN_SIMILARITY", "0.25"))
        self.embedding_model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
        
        # æ‰¹é‡å¤„ç†é…ç½®
        self.max_batch_tokens = 10000  # ç•™ä¸€äº›ä½™é‡ï¼ŒAPIé™åˆ¶æ˜¯20k
        self.api_rate_limit = 50  # æ¯åˆ†é’Ÿ20æ¬¡è¯·æ±‚
        self.last_api_call_times = []  # è®°å½•æœ€è¿‘çš„APIè°ƒç”¨æ—¶é—´
        
        # å¤šæ¨¡æ€é…ç½®
        self.multimodal_enabled = os.getenv("MULTIMODAL_RAG_ENABLED", "true").lower() == "true"
        self.image_storage_path = os.getenv("IMAGE_STORAGE_PATH", "./rag_data/images")
        self.multimodal_search_mode = os.getenv("MULTIMODAL_SEARCH_MODE", "hybrid")
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self._init_vector_db()
        
        # åˆå§‹åŒ–embeddingå®¢æˆ·ç«¯
        self._init_embedding_client()
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
        self._init_text_splitter()
        self.indexer = RAGIndexer(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)
        
        # åˆå§‹åŒ–å¤šæ¨¡æ€å¤„ç†å™¨
        if self.multimodal_enabled:
            self.multimodal_handler = MultimodalEmbeddingHandler(
                client=self.embedding_client,
                model=self.embedding_model
            )
            # ç¡®ä¿å›¾ç‰‡å­˜å‚¨ç›®å½•å­˜åœ¨
            os.makedirs(self.image_storage_path, exist_ok=True)
        
        # åŠ è½½æç¤ºè¯æ¨¡æ¿
        self._load_prompt_templates()
        
    def _init_vector_db(self):
        """åˆå§‹åŒ–ChromaDBå‘é‡æ•°æ®åº“"""
        try:
            # ç¡®ä¿æ•°æ®åº“ç›®å½•å­˜åœ¨
            os.makedirs(self.db_path, exist_ok=True)
            
            # åˆ›å»ºChromaDBå®¢æˆ·ç«¯
            self.chroma_client = chromadb.PersistentClient(
                path=self.db_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # è·å–æˆ–åˆ›å»ºé›†åˆ
            self.collection = self.chroma_client.get_or_create_collection(
                name="knowledge_base",
                metadata={"description": "ç­”ç–‘æœºå™¨äººçŸ¥è¯†åº“"}
            )
            
            print(f"âœ… å‘é‡æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ: {self.db_path}")
            
        except Exception as e:
            print(f"âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
            
    def _init_embedding_client(self):
        """åˆå§‹åŒ–Embedding APIå®¢æˆ·ç«¯"""
        # ä¼˜å…ˆä½¿ç”¨ä¸“é—¨çš„embeddingé…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é€šç”¨OpenAIé…ç½®
        api_key = os.getenv("EMBEDDING_API_KEY") or os.getenv("OPENAI_API_KEY")
        api_base = os.getenv("EMBEDDING_API_BASE") or os.getenv("OPENAI_API_BASE_URL")
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        print(f"ğŸ”§ [RAG] åˆå§‹åŒ–Embeddingå®¢æˆ·ç«¯:")
        print(f"   - EMBEDDING_API_KEY: {'å·²è®¾ç½®' if os.getenv('EMBEDDING_API_KEY') else 'æœªè®¾ç½®'}")
        print(f"   - EMBEDDING_API_BASE: {os.getenv('EMBEDDING_API_BASE') or 'æœªè®¾ç½®'}")
        print(f"   - å®é™…ä½¿ç”¨çš„API Key: {'EMBEDDING_API_KEY' if os.getenv('EMBEDDING_API_KEY') else 'OPENAI_API_KEY'}")
        print(f"   - å®é™…ä½¿ç”¨çš„Base URL: {api_base}")
        
        if not api_key:
            raise ValueError("æœªé…ç½®EMBEDDING_API_KEYæˆ–OPENAI_API_KEY")
        
        if not api_base:
            print("âš ï¸ [RAG] è­¦å‘Šï¼šbase_urlä¸ºç©ºï¼Œè¿™ä¼šå¯¼è‡´è¿æ¥é”™è¯¯ï¼")
            
        self.embedding_client = openai.OpenAI(
            api_key=api_key,
            base_url=api_base
        )
        print(f"âœ… [RAG] Embeddingå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        
    def _init_text_splitter(self):
        """åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨"""
        # ä½¿ç”¨tiktokenè®¡ç®—tokenæ•°é‡
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # åˆ›å»ºé€’å½’å­—ç¬¦åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size * 4,  # ç²—ç•¥ä¼°è®¡ï¼š1 token â‰ˆ 4 characters
            chunk_overlap=self.chunk_overlap * 4,
            length_function=self._count_tokens,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
        )
        
    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        return len(self.encoding.encode(text))
        
    def _load_prompt_templates(self):
        """åŠ è½½æç¤ºè¯æ¨¡æ¿æ–‡ä»¶"""
        try:
            # åŠ è½½ç³»ç»Ÿæç¤ºè¯ï¼ˆå¤´éƒ¨ï¼‰
            app_head_path = "./rag_prompt/app_head.txt"
            if os.path.exists(app_head_path):
                with open(app_head_path, 'r', encoding='utf-8') as f:
                    self.prompt_head = f.read().strip()
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶ {app_head_path}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                self.prompt_head = "åŸºäºä»¥ä¸‹ç›¸å…³çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼š"
                
            # åŠ è½½å…œåº•æç¤ºè¯ï¼ˆå°¾éƒ¨ï¼‰
            app_end_path = "./rag_prompt/app_end.txt"
            if os.path.exists(app_end_path):
                with open(app_end_path, 'r', encoding='utf-8') as f:
                    self.prompt_end = f.read().strip()
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ°å…œåº•æç¤ºè¯æ–‡ä»¶ {app_end_path}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                self.prompt_end = "è¯·æ ¹æ®æä¾›çš„ç›¸å…³çŸ¥è¯†å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœç›¸å…³çŸ¥è¯†ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚"
                
            print(f"âœ… æç¤ºè¯æ¨¡æ¿åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ åŠ è½½æç¤ºè¯æ¨¡æ¿å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤å€¼
            self.prompt_head = "åŸºäºä»¥ä¸‹ç›¸å…³çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ï¼š"
            self.prompt_end = "è¯·æ ¹æ®æä¾›çš„ç›¸å…³çŸ¥è¯†å‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœç›¸å…³çŸ¥è¯†ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚"
        
    async def _wait_for_rate_limit(self):
        """ç­‰å¾…ä»¥éµå®ˆAPIé€Ÿç‡é™åˆ¶"""
        now = time.time()
        # æ¸…ç†è¶…è¿‡60ç§’çš„è®°å½•
        self.last_api_call_times = [t for t in self.last_api_call_times if now - t < 60]
        
        # å¦‚æœè¿‡å»60ç§’å†…çš„è¯·æ±‚æ•°è¾¾åˆ°é™åˆ¶ï¼Œç­‰å¾…
        if len(self.last_api_call_times) >= self.api_rate_limit:
            wait_time = 60 - (now - self.last_api_call_times[0]) + 1  # é¢å¤–ç­‰å¾…1ç§’
            if wait_time > 0:
                print(f"â³ è¾¾åˆ°APIé€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time:.1f} ç§’...")
                await asyncio.sleep(wait_time)
                # é‡æ–°è®¡ç®—
                now = time.time()
                self.last_api_call_times = [t for t in self.last_api_call_times if now - t < 60]
        
        # è®°å½•æœ¬æ¬¡è°ƒç”¨æ—¶é—´
        self.last_api_call_times.append(now)
    
    async def get_embedding(self, content: Union[str, bytes], content_type: Optional[str] = None) -> List[float]:
        """
        è·å–å†…å®¹çš„å‘é‡è¡¨ç¤ºï¼ˆæ”¯æŒæ–‡æœ¬å’Œå›¾ç‰‡ï¼‰
        
        Args:
            content: è¦å‘é‡åŒ–çš„å†…å®¹ï¼ˆæ–‡æœ¬å­—ç¬¦ä¸²æˆ–å›¾ç‰‡å­—èŠ‚ï¼‰
            content_type: å†…å®¹ç±»å‹ï¼ˆ"text"æˆ–"image"ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨æ£€æµ‹
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        if self.multimodal_enabled and isinstance(content, bytes):
            # ä½¿ç”¨å¤šæ¨¡æ€å¤„ç†å™¨å¤„ç†å›¾ç‰‡
            return await self.multimodal_handler.get_embedding(
                content,
                ContentType.IMAGE if content_type == "image" else None
            )
        elif isinstance(content, str):
            # å¤„ç†æ–‡æœ¬
            return (await self.get_embeddings_batch([content]))[0]
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å†…å®¹ç±»å‹: {type(content)}")
    
    async def get_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º
        
        Args:
            texts: è¦å‘é‡åŒ–çš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨çš„åˆ—è¡¨
        """
        try:
            # ç­‰å¾…é€Ÿç‡é™åˆ¶
            await self._wait_for_rate_limit()
            
            # è°ƒè¯•æ—¥å¿—
            print(f"ğŸ”„ [RAG] æ­£åœ¨è°ƒç”¨embedding API:")
            print(f"   - æ¨¡å‹: {self.embedding_model}")
            print(f"   - æ–‡æœ¬æ•°é‡: {len(texts)}")
            print(f"   - Base URL: {self.embedding_client.base_url}")
            
            # ä½¿ç”¨asyncioè¿è¡ŒåŒæ­¥çš„OpenAI APIè°ƒç”¨
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.embedding_client.embeddings.create(
                    model=self.embedding_model,
                    input=texts
                )
            )
            
            # æŒ‰ç…§è¾“å…¥é¡ºåºè¿”å›embeddings
            return [item.embedding for item in response.data]
            
        except Exception as e:
            if "429" in str(e):
                print(f"âš ï¸ é‡åˆ°429é”™è¯¯ï¼Œç­‰å¾…60ç§’åé‡è¯•...")
                await asyncio.sleep(60)
                # é€’å½’é‡è¯•
                return await self.get_embeddings_batch(texts)
            else:
                print(f"âŒ æ‰¹é‡è·å–embeddingå¤±è´¥: {e}")
                raise
            
    def split_text(self, text: str, metadata: Optional[Dict] = None) -> List[Dict]:
        """
        ä½¿ç”¨RAGIndexerè¿›è¡Œæ™ºèƒ½åˆ†å—
        
        Args:
            text: è¦åˆ†å—çš„æ–‡æœ¬
            metadata: é¢å¤–çš„å…ƒæ•°æ®
            
        Returns:
            åŒ…å«æ–‡æœ¬å—å’Œå…ƒæ•°æ®çš„å­—å…¸åˆ—è¡¨
        """
        chunks = self.indexer.smart_split(text, metadata)
        
        # ä¸ºæ¯ä¸ªå—ç”Ÿæˆå”¯ä¸€çš„IDå¹¶æ ¼å¼åŒ–
        result = []
        for i, chunk_data in enumerate(chunks):
            chunk_text = chunk_data["text"]
            chunk_metadata = chunk_data["metadata"]
            
            # æ›´æ–°å…ƒæ•°æ®
            chunk_metadata["chunk_index"] = i
            chunk_metadata["chunk_total"] = len(chunks)
            
            # æ¸…ç†metadataï¼Œç¡®ä¿ChromaDBå…¼å®¹æ€§ï¼ˆåªæ”¯æŒstr, int, float, bool, Noneï¼‰
            cleaned_metadata = {}
            for key, value in chunk_metadata.items():
                if isinstance(value, list):
                    # å°†åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                    if value:  # éç©ºåˆ—è¡¨
                        if isinstance(value[0], tuple):
                            # å¤„ç†parent_titlesè¿™æ ·çš„å…ƒç»„åˆ—è¡¨
                            cleaned_metadata[key] = ",".join([str(item[1]) if len(item) > 1 else str(item[0]) for item in value])
                        else:
                            cleaned_metadata[key] = ",".join([str(item) for item in value])
                    else:
                        cleaned_metadata[key] = ""
                elif isinstance(value, (str, int, float, bool)) or value is None:
                    cleaned_metadata[key] = value
                else:
                    # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    cleaned_metadata[key] = str(value)
            
            chunk_metadata = cleaned_metadata
            
            # ç”Ÿæˆå—çš„å”¯ä¸€ID - åŒ…å«æºæ–‡ä»¶ã€ç´¢å¼•å’Œæ—¶é—´æˆ³ç¡®ä¿å”¯ä¸€æ€§
            source_info = chunk_metadata.get("source", "unknown")
            timestamp = str(int(time.time() * 1000))  # æ¯«ç§’çº§æ—¶é—´æˆ³
            unique_string = f"{source_info}_{i}_{timestamp}_{chunk_text[:100]}"
            chunk_id = hashlib.md5(unique_string.encode()).hexdigest()
            
            result.append({
                "id": chunk_id,
                "text": chunk_text,
                "metadata": chunk_metadata
            })
            
        return result
        
    async def index_document(self, text: str, source: str = "unknown") -> int:
        """
        ç´¢å¼•æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            text: æ–‡æ¡£æ–‡æœ¬
            source: æ–‡æ¡£æ¥æº
            
        Returns:
            ç´¢å¼•çš„å—æ•°é‡
        """
        try:
            # åˆ†å—æ–‡æ¡£
            chunks = self.split_text(text, metadata={"source": source, "content_type": "text"})
            
            if not chunks:
                print("âš ï¸ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æœ¬å—")
                return 0
                
            print(f"ğŸ“ æ­£åœ¨ç´¢å¼• {len(chunks)} ä¸ªæ–‡æœ¬å—...")
            
            # å°†chunksåˆ†ç»„ä¸ºæ‰¹æ¬¡ï¼Œç¡®ä¿æ¯æ‰¹çš„æ€»tokenæ•°ä¸è¶…è¿‡é™åˆ¶
            batches = []
            current_batch = []
            current_tokens = 0
            
            for chunk in chunks:
                chunk_tokens = chunk["metadata"]["tokens"]
                
                # å¦‚æœå½“å‰æ‰¹æ¬¡åŠ ä¸Šè¿™ä¸ªchunkä¼šè¶…è¿‡é™åˆ¶ï¼Œåˆ™å¼€å§‹æ–°æ‰¹æ¬¡
                if current_tokens + chunk_tokens > self.max_batch_tokens and current_batch:
                    batches.append(current_batch)
                    current_batch = [chunk]
                    current_tokens = chunk_tokens
                else:
                    current_batch.append(chunk)
                    current_tokens += chunk_tokens
            
            # æ·»åŠ æœ€åä¸€ä¸ªæ‰¹æ¬¡
            if current_batch:
                batches.append(current_batch)
            
            print(f"ğŸ“¦ åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡è¿›è¡Œå¤„ç†")
            
            # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
            all_ids = []
            all_texts = []
            all_embeddings = []
            all_metadatas = []
            
            for batch_idx, batch in enumerate(batches, 1):
                print(f"  å¤„ç†æ‰¹æ¬¡ {batch_idx}/{len(batches)}ï¼š{len(batch)} ä¸ªæ–‡æœ¬å—ï¼Œ"
                      f"çº¦ {sum(c['metadata']['tokens'] for c in batch)} tokens")
                
                # æ‰¹é‡è·å–embeddings
                batch_texts = [chunk["text"] for chunk in batch]
                batch_embeddings = await self.get_embeddings_batch(batch_texts)
                
                # æ”¶é›†æ•°æ®
                for chunk, embedding in zip(batch, batch_embeddings):
                    all_ids.append(chunk["id"])
                    all_texts.append(chunk["text"])
                    all_embeddings.append(embedding)
                    all_metadatas.append(chunk["metadata"])
            
            # ä¸€æ¬¡æ€§æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.collection.add(
                ids=all_ids,
                documents=all_texts,
                embeddings=all_embeddings,
                metadatas=all_metadatas
            )
            
            print(f"âœ… æˆåŠŸç´¢å¼• {len(chunks)} ä¸ªæ–‡æœ¬å—")
            return len(chunks)
            
        except Exception as e:
            print(f"âŒ ç´¢å¼•æ–‡æ¡£å¤±è´¥: {e}")
            raise
            
    async def index_image(
        self,
        image_data: bytes,
        source: str = "unknown",
        text_description: Optional[str] = None,
        metadata: Optional[Dict] = None
    ) -> str:
        """
        ç´¢å¼•å›¾ç‰‡åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            image_data: å›¾ç‰‡å­—èŠ‚æ•°æ®
            source: å›¾ç‰‡æ¥æº
            text_description: å›¾ç‰‡çš„æ–‡æœ¬æè¿°ï¼ˆå¯é€‰ï¼‰
            metadata: é¢å¤–çš„å…ƒæ•°æ®
            
        Returns:
            å›¾ç‰‡ID
        """
        if not self.multimodal_enabled:
            raise ValueError("å¤šæ¨¡æ€åŠŸèƒ½æœªå¯ç”¨")
            
        try:
            # ç”Ÿæˆå›¾ç‰‡ID
            image_id = hashlib.md5(image_data).hexdigest()[:16]
            
            # ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°
            image_filename = f"{image_id}.jpg"
            image_path = os.path.join(self.image_storage_path, image_filename)
            
            # é¢„å¤„ç†å¹¶ä¿å­˜å›¾ç‰‡
            processed_image = await self.multimodal_handler._preprocess_image(image_data)
            with open(image_path, 'wb') as f:
                f.write(processed_image)
            
            print(f"ğŸ–¼ï¸ æ­£åœ¨ç´¢å¼•å›¾ç‰‡: {image_filename}")
            
            # è·å–å›¾ç‰‡embedding
            image_embedding = await self.multimodal_handler.get_embedding(
                processed_image, ContentType.IMAGE
            )
            
            # å‡†å¤‡å…ƒæ•°æ®
            image_metadata = {
                "source": source,
                "content_type": "image",
                "image_path": image_path,
                "image_filename": image_filename,
                "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
                **(metadata or {})
            }
            
            # å¦‚æœæ²¡æœ‰æä¾›æè¿°ï¼Œä½¿ç”¨é»˜è®¤æè¿°
            if not text_description:
                text_description = f"Image from {source}"
            
            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            self.collection.add(
                ids=[image_id],
                documents=[text_description],
                embeddings=[image_embedding],
                metadatas=[image_metadata]
            )
            
            print(f"âœ… æˆåŠŸç´¢å¼•å›¾ç‰‡: {image_id}")
            return image_id
            
        except Exception as e:
            print(f"âŒ ç´¢å¼•å›¾ç‰‡å¤±è´¥: {e}")
            raise
            
    async def index_multimodal_document(
        self,
        document: MultimodalDocument,
        source: str = "unknown"
    ) -> Dict[str, any]:
        """
        ç´¢å¼•å¤šæ¨¡æ€æ–‡æ¡£ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡ï¼‰
        
        Args:
            document: å¤šæ¨¡æ€æ–‡æ¡£å¯¹è±¡
            source: æ–‡æ¡£æ¥æº
            
        Returns:
            ç´¢å¼•ç»“æœç»Ÿè®¡
        """
        if not self.multimodal_enabled:
            # å¦‚æœå¤šæ¨¡æ€æœªå¯ç”¨ï¼Œåªç´¢å¼•æ–‡æœ¬éƒ¨åˆ†
            if document.has_text():
                chunks_count = await self.index_document(document.text, source)
                return {"text_chunks": chunks_count, "images": 0}
            else:
                return {"text_chunks": 0, "images": 0}
                
        try:
            results = {"text_chunks": 0, "images": 0, "mixed_chunks": 0}
            
            # ä¿å­˜å›¾ç‰‡
            image_paths = []
            if document.has_images():
                image_paths = await document.save_images(self.image_storage_path)
                
            # å¦‚æœæ˜¯çº¯æ–‡æœ¬æ–‡æ¡£
            if document.has_text() and not document.has_images():
                results["text_chunks"] = await self.index_document(document.text, source)
                
            # å¦‚æœæ˜¯çº¯å›¾ç‰‡æ–‡æ¡£
            elif document.has_images() and not document.has_text():
                for i, (image_data, image_path) in enumerate(zip(document.images, image_paths)):
                    await self.index_image(
                        image_data,
                        source,
                        f"Image {i+1} from document {document.doc_id}",
                        {"document_id": document.doc_id, "image_index": i}
                    )
                    results["images"] += 1
                    
            # å¦‚æœæ˜¯æ··åˆæ–‡æ¡£
            elif document.is_multimodal():
                # åˆ†å—æ–‡æœ¬ï¼Œä½†æ·»åŠ å›¾ç‰‡å¼•ç”¨ä¿¡æ¯
                chunks = self.split_text(
                    document.text,
                    metadata={
                        "source": source,
                        "content_type": "mixed",
                        "document_id": document.doc_id,
                        "has_images": True,
                        "image_count": len(document.images)
                    }
                )
                
                # ç´¢å¼•æ–‡æœ¬å—
                for chunk in chunks:
                    # ChromaDBä¸æ”¯æŒåˆ—è¡¨ç±»å‹çš„metadataï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    if image_paths:
                        # å°†å›¾ç‰‡è·¯å¾„åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                        chunk["metadata"]["associated_images"] = ",".join(image_paths)
                        chunk["metadata"]["associated_images_count"] = len(image_paths)
                    
                # æ‰¹é‡å¤„ç†æ–‡æœ¬å—ï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰
                if chunks:
                    # è·å–embeddingså¹¶æ·»åŠ åˆ°æ•°æ®åº“
                    texts = [chunk["text"] for chunk in chunks]
                    embeddings = await self.get_embeddings_batch(texts)
                    
                    self.collection.add(
                        ids=[chunk["id"] for chunk in chunks],
                        documents=texts,
                        embeddings=embeddings,
                        metadatas=[chunk["metadata"] for chunk in chunks]
                    )
                    
                    results["text_chunks"] = len(chunks)
                    
                # åŒæ—¶ç´¢å¼•å›¾ç‰‡ï¼Œå…³è”åˆ°æ–‡æ¡£
                for i, (image_data, image_path) in enumerate(zip(document.images, image_paths)):
                    await self.index_image(
                        image_data,
                        source,
                        f"Image {i+1} associated with text: {document.text[:100]}...",
                        {
                            "document_id": document.doc_id,
                            "image_index": i,
                            "associated_text": document.text[:200]
                        }
                    )
                    results["images"] += 1
                    
                results["mixed_chunks"] = results["text_chunks"]
                
            print(f"âœ… æˆåŠŸç´¢å¼•å¤šæ¨¡æ€æ–‡æ¡£: {results}")
            return results
            
        except Exception as e:
            print(f"âŒ ç´¢å¼•å¤šæ¨¡æ€æ–‡æ¡£å¤±è´¥: {e}")
            raise
            
    async def retrieve_context(
        self,
        query: Union[str, bytes, Dict],
        top_k: Optional[int] = None,
        mode: Optional[str] = None
    ) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆæ”¯æŒå¤šæ¨¡æ€æŸ¥è¯¢ï¼‰
        
        Args:
            query: æŸ¥è¯¢å†…å®¹ - å¯ä»¥æ˜¯æ–‡æœ¬ã€å›¾ç‰‡å­—èŠ‚æˆ–åŒ…å«text/imageé”®çš„å­—å…¸
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            mode: æ£€ç´¢æ¨¡å¼ - "text_only", "image_only", "hybrid"
            
        Returns:
            ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨é…ç½®çš„top_kæˆ–é»˜è®¤å€¼
            k = top_k or self.top_k
            search_mode = mode or self.multimodal_search_mode
            
            # è·å–æŸ¥è¯¢çš„embedding
            if isinstance(query, dict) and self.multimodal_enabled:
                # å¤šæ¨¡æ€æŸ¥è¯¢
                query_embedding, metadata = await self.multimodal_handler.get_multimodal_embedding(
                    text=query.get("text"),
                    image=query.get("image"),
                    mode=search_mode
                )
            elif isinstance(query, bytes) and self.multimodal_enabled:
                # å›¾ç‰‡æŸ¥è¯¢
                query_embedding = await self.multimodal_handler.get_embedding(
                    query, ContentType.IMAGE
                )
            else:
                # æ–‡æœ¬æŸ¥è¯¢ï¼ˆå‘åå…¼å®¹ï¼‰
                if isinstance(query, dict):
                    query = query.get("text", "")
                query_embeddings = await self.get_embeddings_batch([query])
                query_embedding = query_embeddings[0]
            
            # å‘é‡ç›¸ä¼¼åº¦æœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=k,
                include=["documents", "metadatas", "distances"]
            )
            
            # å¤„ç†ç»“æœ
            contexts = []
            if results["documents"] and len(results["documents"]) > 0:
                for i, doc in enumerate(results["documents"][0]):
                    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆChromaDBè¿”å›çš„æ˜¯è·ç¦»ï¼Œéœ€è¦è½¬æ¢ï¼‰
                    distance = results["distances"][0][i] if results["distances"] else 0
                    similarity = 1 - distance  # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—
                    
                    # è¿‡æ»¤ä½ç›¸ä¼¼åº¦çš„ç»“æœ
                    if similarity >= self.min_similarity:
                        metadata = results["metadatas"][0][i] if results["metadatas"] else {}
                        context = {
                            "text": doc,
                            "metadata": metadata,
                            "similarity": similarity
                        }
                        
                        # å¦‚æœæ˜¯å›¾ç‰‡ç»“æœï¼Œæ·»åŠ å›¾ç‰‡è·¯å¾„ä¿¡æ¯
                        if metadata.get("content_type") == "image" and metadata.get("image_path"):
                            context["image_path"] = metadata["image_path"]
                            
                        contexts.append(context)
                        
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            contexts.sort(key=lambda x: x["similarity"], reverse=True)
            
            return contexts
            
        except Exception as e:
            print(f"âŒ æ£€ç´¢ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return []
            
    async def build_enhanced_prompt(self, query: Union[str, Dict], contexts: List[Dict]) -> str:
        """
        æ„å»ºå¢å¼ºçš„æç¤ºè¯ï¼ˆæ”¯æŒå¤šæ¨¡æ€ä¸Šä¸‹æ–‡ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢ï¼ˆæ–‡æœ¬æˆ–åŒ…å«text/imageçš„å­—å…¸ï¼‰
            contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            
        Returns:
            å¢å¼ºåçš„æç¤ºè¯
        """
        if not contexts:
            # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼Œè¿”å›åŸå§‹æŸ¥è¯¢
            if isinstance(query, dict):
                return query.get("text", "")
            return query
            
        # æå–æŸ¥è¯¢æ–‡æœ¬
        query_text = query if isinstance(query, str) else query.get("text", "")
        
        # æ„å»ºä¸Šä¸‹æ–‡éƒ¨åˆ†
        context_parts = []
        image_references = []
        
        for i, ctx in enumerate(contexts, 1):
            # å¤„ç†æ–‡æœ¬ä¸Šä¸‹æ–‡
            context_parts.append(f"[ç›¸å…³çŸ¥è¯† {i}]\n{ctx['text']}\n")
            
            # å¦‚æœæœ‰å…³è”çš„å›¾ç‰‡ï¼Œæ·»åŠ å¼•ç”¨
            if ctx.get("image_path"):
                image_references.append(f"[å›¾ç‰‡ {i}]: {ctx['image_path']}")
            elif ctx["metadata"].get("associated_images"):
                # associated_images ç°åœ¨æ˜¯é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                img_paths = ctx["metadata"]["associated_images"].split(",")
                for img_path in img_paths:
                    image_references.append(f"[å…³è”å›¾ç‰‡]: {img_path.strip()}")
                    
        # æ„å»ºå›¾ç‰‡å¼•ç”¨éƒ¨åˆ†
        image_section = ""
        if image_references:
            image_section = "\n[ç›¸å…³å›¾ç‰‡èµ„æº]\n" + "\n".join(image_references) + "\n"
            
        # ç»„åˆå¢å¼ºæç¤ºè¯ï¼Œä½¿ç”¨ä»æ–‡ä»¶åŠ è½½çš„æ¨¡æ¿
        # æ³¨æ„ï¼šä¸åœ¨ç³»ç»Ÿæç¤ºè¯ä¸­åŒ…å«ç”¨æˆ·é—®é¢˜ï¼Œç”¨æˆ·é—®é¢˜åº”è¯¥åœ¨userè§’è‰²çš„æ¶ˆæ¯ä¸­
        enhanced_prompt = f"""{self.prompt_head}

[çŸ¥è¯†åº“å¼€å§‹]
{''.join(context_parts)}
{image_section}
{self.prompt_end}"""
        
        return enhanced_prompt
        
    def get_stats(self) -> Dict:
        """
        è·å–RAGç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            # è·å–é›†åˆç»Ÿè®¡
            count = self.collection.count()
            
            stats = {
                "status": "active",
                "database_path": self.db_path,
                "total_chunks": count,
                "embedding_model": self.embedding_model,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
                "top_k": self.top_k,
                "min_similarity": self.min_similarity,
                "max_batch_tokens": self.max_batch_tokens,
                "api_rate_limit": f"{self.api_rate_limit} requests/min",
                "recent_api_calls": len(self.last_api_call_times)
            }
            
            # æ·»åŠ å¤šæ¨¡æ€ç›¸å…³ç»Ÿè®¡
            if self.multimodal_enabled:
                stats.update({
                    "multimodal_enabled": True,
                    "image_storage_path": self.image_storage_path,
                    "multimodal_search_mode": self.multimodal_search_mode
                })
                
                # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
                if os.path.exists(self.image_storage_path):
                    image_count = len([f for f in os.listdir(self.image_storage_path)
                                     if f.endswith(('.jpg', '.jpeg', '.png', '.gif'))])
                    stats["stored_images"] = image_count
                    
            return stats
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
            
    def clear_database(self):
        """æ¸…ç©ºå‘é‡æ•°æ®åº“"""
        try:
            # åˆ é™¤å¹¶é‡æ–°åˆ›å»ºé›†åˆ
            self.chroma_client.delete_collection("knowledge_base")
            self.collection = self.chroma_client.create_collection(
                name="knowledge_base",
                metadata={"description": "ç­”ç–‘æœºå™¨äººçŸ¥è¯†åº“"}
            )
            print("âœ… å‘é‡æ•°æ®åº“å·²æ¸…ç©º")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºæ•°æ®åº“å¤±è´¥: {e}")
            raise


# ç®€å•çš„æ–‡æ¡£åˆ†å—å·¥å…·å‡½æ•°
def simple_chunk_text(text: str, max_tokens: int = 500, overlap: int = 50) -> List[str]:
    """
    ç®€å•çš„æ–‡æ¡£åˆ†å—åŠŸèƒ½
    
    Args:
        text: è¦åˆ†å—çš„æ–‡æœ¬
        max_tokens: æ¯å—çš„æœ€å¤§tokenæ•°
        overlap: å—ä¹‹é—´çš„é‡å tokenæ•°
        
    Returns:
        æ–‡æœ¬å—åˆ—è¡¨
    """
    # ä½¿ç”¨tiktokenç¼–ç 
    encoding = tiktoken.get_encoding("cl100k_base")
    
    # å°†æ–‡æœ¬ç¼–ç ä¸ºtokens
    tokens = encoding.encode(text)
    
    chunks = []
    start = 0
    
    while start < len(tokens):
        # è®¡ç®—å—çš„ç»“æŸä½ç½®
        end = min(start + max_tokens, len(tokens))
        
        # æå–tokenå—å¹¶è§£ç å›æ–‡æœ¬
        chunk_tokens = tokens[start:end]
        chunk_text = encoding.decode(chunk_tokens)
        chunks.append(chunk_text)
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå—çš„å¼€å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
        start = end - overlap if end < len(tokens) else end
        
    return chunks


# æµ‹è¯•å‡½æ•°
async def test_rag_processor():
    """æµ‹è¯•RAGå¤„ç†å™¨åŠŸèƒ½"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•RAGå¤„ç†å™¨...")
    
    # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
    processor = RAGProcessor()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    # SillyTavern å®‰è£…æŒ‡å—
    
    ## Windowså®‰è£…
    1. ä¸‹è½½æœ€æ–°ç‰ˆæœ¬çš„å®‰è£…åŒ…
    2. è§£å‹åˆ°ä»»æ„ç›®å½•
    3. è¿è¡Œstart.batæ–‡ä»¶
    
    ## å¸¸è§é”™è¯¯
    
    ### ETIMEDOUTé”™è¯¯
    è¿™ä¸ªé”™è¯¯é€šå¸¸è¡¨ç¤ºç½‘ç»œè¿æ¥è¶…æ—¶ã€‚è§£å†³æ–¹æ³•ï¼š
    - æ£€æŸ¥ç½‘ç»œè¿æ¥
    - ä½¿ç”¨ä»£ç†
    - é‡è¯•æ“ä½œ
    
    ### 429é”™è¯¯
    è¿™æ˜¯APIé€Ÿç‡é™åˆ¶é”™è¯¯ã€‚è§£å†³æ–¹æ³•ï¼š
    - é™ä½è¯·æ±‚é¢‘ç‡
    - ç­‰å¾…ä¸€æ®µæ—¶é—´å†è¯•
    """
    
    # æµ‹è¯•åˆ†å—
    print("\nğŸ“„ æµ‹è¯•æ–‡æ¡£åˆ†å—...")
    chunks = processor.split_text(test_text)
    print(f"ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
    
    # æµ‹è¯•ç´¢å¼•
    print("\nğŸ“¥ æµ‹è¯•æ–‡æ¡£ç´¢å¼•...")
    chunk_count = await processor.index_document(test_text, source="test")
    print(f"ç´¢å¼•äº† {chunk_count} ä¸ªæ–‡æœ¬å—")
    
    # æµ‹è¯•æ£€ç´¢
    print("\nğŸ” æµ‹è¯•ä¸Šä¸‹æ–‡æ£€ç´¢...")
    contexts = await processor.retrieve_context("ETIMEDOUTé”™è¯¯æ€ä¹ˆè§£å†³ï¼Ÿ")
    print(f"æ£€ç´¢åˆ° {len(contexts)} ä¸ªç›¸å…³æ–‡æ¡£")
    
    if contexts:
        print(f"æœ€ç›¸å…³çš„æ–‡æ¡£ï¼ˆç›¸ä¼¼åº¦: {contexts[0]['similarity']:.2f}ï¼‰:")
        print(contexts[0]['text'][:200] + "...")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
    stats = processor.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")


class RAGProcessorCog(commands.Cog):
    """RAGå¤„ç†å™¨Cogï¼Œæä¾›æ–‡æ¡£å‘é‡åŒ–å’Œæ£€ç´¢åŠŸèƒ½"""
    
    def __init__(self, bot):
        self.bot = bot
        self.processor = RAGProcessor()
    
    @commands.command(name="test_processor")
    async def test_processor_command(self, ctx):
        """æµ‹è¯•RAGå¤„ç†å™¨åŠŸèƒ½"""
        stats = self.processor.get_stats()
        await ctx.send(f"RAGå¤„ç†å™¨åŠŸèƒ½æ­£å¸¸è¿è¡Œ\næ•°æ®åº“ç»Ÿè®¡: {stats}")

async def setup(bot):
    """è®¾ç½®å‡½æ•°ï¼Œç”¨äºåŠ è½½cog"""
    await bot.add_cog(RAGProcessorCog(bot))

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_rag_processor())