"""  
RAG æ™ºèƒ½ç´¢å¼•å™¨æ¨¡å—
è´Ÿè´£çŸ¥è¯†åº“æ–‡æ¡£çš„æ™ºèƒ½åˆ†å—
"""

import re
from typing import List, Dict, Optional, Tuple
import tiktoken
import discord
from discord.ext import commands

class RAGIndexer:
    """
    RAGæ™ºèƒ½ç´¢å¼•å™¨ï¼Œè´Ÿè´£å°†æ–‡æ¡£è¿›è¡Œè¯­ä¹‰åŒ–ã€ç»“æ„åŒ–çš„æ™ºèƒ½åˆ†å—ã€‚
    """
    
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        åˆå§‹åŒ–ç´¢å¼•å™¨
        
        Args:
            chunk_size: ç›®æ ‡å—å¤§å° (tokens)
            chunk_overlap: å—ä¹‹é—´çš„é‡å å¤§å° (tokens)
        """
        self.target_chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.encoding = tiktoken.get_encoding("cl100k_base")

    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        return len(self.encoding.encode(text, disallowed_special=()))

    def smart_split(self, text: str, metadata: Optional[Dict] = None) -> List[Dict]:
        """
        æ™ºèƒ½åˆ†å—ä¸»æ–¹æ³• - é‡‡ç”¨ä¸¤çº§åˆ†å‰²ç­–ç•¥
        """
        # 1. é¢„å¤„ç†ï¼šæ¸…ç†æ–‡æœ¬
        text = self._preprocess_text(text)
        
        # 2. ç¬¬ä¸€çº§åˆ†å‰²ï¼šæŒ‰ '===' åˆ†å‰²æˆä¸»è¦éƒ¨åˆ†
        major_sections = re.split(r'\n\s*===\s*\n', text)
        
        all_sub_sections = []
        for section in major_sections:
            section = section.strip()
            if not section:
                continue
            
            # æå–è¯¥ä¸»è¦éƒ¨åˆ†çš„é¡¶çº§æ ‡é¢˜
            top_level_title_match = re.match(r'#\s*\[([^\]]+)\]', section)
            top_level_title = top_level_title_match.group(1) if top_level_title_match else "General"
            
            # 3. ç¬¬äºŒçº§åˆ†å‰²ï¼šåœ¨æ¯ä¸ªä¸»è¦éƒ¨åˆ†å†…éƒ¨è¿›è¡Œç»“æ„åŒ–åˆ†å‰²
            sub_sections = self._split_by_structural_separators(section, top_level_title)
            all_sub_sections.extend(sub_sections)
            
        # 4. è¿›ä¸€æ­¥å¤„ç†å’Œç»†åˆ†æ‰€æœ‰å­ç« èŠ‚
        final_chunks = []
        for section in all_sub_sections:
            section_content = section['content']
            
            # ç»§æ‰¿çˆ¶çº§å…ƒæ•°æ®å¹¶æ·»åŠ å½“å‰ç« èŠ‚ä¿¡æ¯
            section_metadata = {
                **(metadata or {}),
                'content_type': self._determine_content_type(section_content),
                'title': section['title'],
                'parent_titles': section['parents']
            }
            
            # æ£€æŸ¥ç« èŠ‚å¤§å°
            section_tokens = self._count_tokens(section_content)
            
            if section_tokens <= self.target_chunk_size * 1.2: # å…è®¸20%çš„è¶…é¢
                final_chunks.append({
                    "text": section_content,
                    "metadata": section_metadata
                })
            else:
                # å¦‚æœç« èŠ‚å¤ªå¤§ï¼Œåˆ™è¿›è¡Œæ›´ç»†ç²’åº¦çš„åˆ†å‰²
                sub_chunks = self._split_large_section(section_content, section_metadata)
                final_chunks.extend(sub_chunks)
        
        # 5. æ·»åŠ é‡å å¹¶æ ¼å¼åŒ–è¾“å‡º
        return self._format_chunks_with_overlap(final_chunks)

    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬ï¼Œæ¸…ç†å¤šä½™çš„ç©ºè¡Œç­‰"""
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text.strip()

    def _split_by_structural_separators(self, text: str, top_level_parent: str) -> List[Dict]:
        """
        ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŒ‰Markdownæ ‡é¢˜å’Œ'---'åˆ†éš”ç¬¦åœ¨å•ä¸ªä¸»åŒºåŸŸå†…åˆ†å‰²æ–‡æœ¬ã€‚
        """
        # åˆ†éš”ç¬¦ï¼š'---' æˆ– '#', '##', '###' æ ‡é¢˜
        separator_pattern = re.compile(r'(^\s*---\s*$|^#{1,3}\s+.*$)', re.MULTILINE)
        
        separators = list(separator_pattern.finditer(text))
        
        if not separators:
            # å¦‚æœæ²¡æœ‰åˆ†éš”ç¬¦ï¼Œæ•´ä¸ªåŒºåŸŸä½œä¸ºä¸€ä¸ªå—
            title_match = re.match(r'#\s*\[([^\]]+)\]', text)
            title = title_match.group(1) if title_match else top_level_parent
            return [{'level': 1, 'title': title, 'content': text, 'parents': [top_level_parent]}]
            
        sections = []
        # çˆ¶æ ‡é¢˜æ ˆï¼š(level, title)
        parent_titles: List[Tuple[int, str]] = [(0, top_level_parent)]

        # å¤„ç†ç¬¬ä¸€ä¸ªåˆ†éš”ç¬¦ä¹‹å‰çš„å†…å®¹
        if separators and separators[0].start() > 0:
            intro_content = text[:separators[0].start()].strip()
            if intro_content:
                # å°è¯•ä»å¼•è¨€ä¸­æå–æ ‡é¢˜
                intro_title_match = re.match(r'#\s*\[([^\]]+)\]', intro_content)
                intro_title = intro_title_match.group(1) if intro_title_match else "Introduction"
                sections.append({
                    'level': 1,
                    'title': intro_title,
                    'content': intro_content,
                    'parents': [p for p in parent_titles]
                })
        
        # éå†æ‰€æœ‰åˆ†éš”ç¬¦ï¼Œåˆ›å»ºç« èŠ‚
        for i, separator in enumerate(separators):
            separator_text = separator.group(1).strip()
            
            level = 0
            title = ""
            
            if separator_text.startswith('---'):
                # --- ä½œä¸ºä¸ ## åŒçº§çš„åˆ†å‰²
                level = 2 
                # å°è¯•ä»å†…å®¹ä¸­æå–æ›´æœ‰æ„ä¹‰çš„æ ‡é¢˜
                start_pos_content = separator.end()
                end_pos_content = separators[i+1].start() if i + 1 < len(separators) else len(text)
                content_preview = text[start_pos_content:end_pos_content].strip()
                first_line = content_preview.split('\n')
                
                # ç‰¹æ®Šå¤„ç†äººç‰©ç®€ä»‹çš„æ ‡é¢˜æå–
                if top_level_parent == 'ç±»è„‘ç¤¾åŒºå†å²äººç‰©':
                     title = self._extract_person_name(content_preview)
                elif '###' in first_line: # å¦‚æœç´§æ¥ç€æ˜¯ä¸‰çº§æ ‡é¢˜ï¼Œç”¨å®ƒ
                    title = first_line.lstrip('#').strip()
                else:
                    title = f"Section"

            elif separator_text.startswith('#'):
                level = separator_text.count('#')
                title = separator_text.lstrip('#').strip().replace('[','').replace(']','') or f"Header Level {level}"

            # æ›´æ–°çˆ¶æ ‡é¢˜æ ˆ - ç§»é™¤æ‰€æœ‰çº§åˆ«å¤§äºç­‰äºå½“å‰çº§åˆ«çš„æ ‡é¢˜
            while parent_titles and parent_titles[-1][0] >= level:
                parent_titles.pop()
            
            current_parents = [p for p in parent_titles]

            start_pos = separator.end()
            end_pos = separators[i+1].start() if i + 1 < len(separators) else len(text)
            
            content = text[start_pos:end_pos].strip()
            
            if not content:
                continue
            
            # å°†åˆ†éš”ç¬¦ï¼ˆæ ‡é¢˜ï¼‰å’Œå†…å®¹åˆå¹¶ï¼Œä¿ç•™å®Œæ•´ä¸Šä¸‹æ–‡
            full_content = f"{separator_text}\n{content}"
            
            sections.append({
                'level': level,
                'title': title,
                'content': full_content,
                'parents': current_parents.copy()
            })

            # å°†å½“å‰æ ‡é¢˜å‹å…¥çˆ¶æ ‡é¢˜æ ˆ
            parent_titles.append((level, title))
            
        return sections

    def _determine_content_type(self, text: str) -> str:
        """æ”¹è¿›çš„å†…å®¹ç±»å‹åˆ¤æ–­é€»è¾‘"""
        lower_text = text.lower()
        
        # æ£€æµ‹Q&Aæ ¼å¼ - æ›´å‡†ç¡®çš„åŒ¹é…
        if re.search(r'^q\s*[ï¼š:]', lower_text, re.MULTILINE) and re.search(r'^a\s*[ï¼š:]', lower_text, re.MULTILINE):
            return "qa"
        
        # æ£€æµ‹æ•…éšœæ’é™¤æ ¼å¼ - æ›´å®½æ¾çš„åŒ¹é…
        troubleshoot_keywords = ['ç°è±¡', 'åŸå› ', 'è§£å†³', 'é—®é¢˜', 'æŠ¥é”™']
        if sum(1 for keyword in troubleshoot_keywords if keyword in text) >= 2:
            return "troubleshooting"
        
        # æ£€æµ‹æ•™ç¨‹å†…å®¹
        tutorial_keywords = ['å®‰è£…', 'æ›´æ–°', 'æ­¥éª¤', 'æ•™ç¨‹', 'æŒ‡å—', 'å¦‚ä½•']
        if sum(1 for keyword in tutorial_keywords if keyword in lower_text) >= 2:
            return "tutorial"
        
        # æ£€æµ‹äººç‰©ä¿¡æ¯
        if self._is_person_info(text):
            return "person"
            
        return "reference"

    def _is_person_info(self, text: str) -> bool:
        """æ”¹è¿›çš„äººç‰©ä¿¡æ¯æ£€æµ‹é€»è¾‘"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«äººç‰©æè¿°å…³é”®è¯
        person_keywords = [
            'æ˜¯ç±»è„‘ç¤¾åŒº', 'ç®¡ç†å‘˜', 'åˆ›ä½œè€…', 'ä½œè€…', 'æœä¸»', 'owner',
            'è®®ä¼š', 'å¼¹åŠ¾', 'è¾èŒ', 'æ´»è·ƒ', 'è´¡çŒ®', 'åˆ¶ä½œ', 'å»ºç«‹',
            '2024å¹´', '2025å¹´', 'æ—©æœŸ', 'å†å²', 'é‡è¦', 'ç¤¾åŒº'
        ]
        
        keyword_count = sum(1 for keyword in person_keywords if keyword in text.lower())
        
        # æ£€æŸ¥äººç‰©ä¿¡æ¯æ ¼å¼æ¨¡å¼
        patterns = [
            r'^[^\n]*\([^)]+\)\s*[ï¼š:]',  # å§“å(åˆ«å):
            r'^[\w\s]+\s*[ï¼š:].*(?:ç®¡ç†|åˆ›ä½œ|ä½œè€…)',  # å§“å: ...ç®¡ç†/åˆ›ä½œ/ä½œè€…
            r'(?:ç®¡ç†å‘˜|åˆ›ä½œè€…|æœä¸»|Owner).*[\w\s]+',  # åŒ…å«èŒä½çš„æè¿°
        ]
        
        pattern_match = any(re.search(pattern, text, re.MULTILINE | re.IGNORECASE) for pattern in patterns)
        
        return keyword_count >= 2 or pattern_match

    def _extract_person_name(self, text: str) -> str:
        """æ”¹è¿›çš„äººç‰©å§“åæå–"""
        if '## ç±»è„‘ç¤¾åŒºå†å²äººç‰©' in text:
            return 'ç±»è„‘ç¤¾åŒºå†å²äººç‰©'
            
        # è·å–ç¬¬ä¸€è¡Œ
        first_line = text.split('\n').strip()
        
        # å°è¯•å¤šç§æ ¼å¼åŒ¹é…
        patterns = [
            r'^([^(ï¼š:]+)(?:\([^)]+\))?\s*[:ï¼š]',  # å§“å(åˆ«å):
            r'^([\w\s]+)(?:\([^)]+\))?\s*[:ï¼š]',   # å§“å:
            r'^([^\n]{1,30}?)(?:\s*[:ï¼š]|$)',      # å‰30å­—ç¬¦ä½œä¸ºæ ‡é¢˜
        ]
        
        for pattern in patterns:
            match = re.match(pattern, first_line)
            if match:
                name = match.group(1).strip()
                if name and len(name) > 0:
                    return name
                    
        # å¦‚æœéƒ½æ²¡åŒ¹é…åˆ°ï¼Œè¿”å›æˆªæ–­çš„ç¬¬ä¸€è¡Œ
        return first_line[:30].strip() or 'Unknown'

    def _split_large_section(self, text: str, metadata: Dict) -> List[Dict]:
        """
        åˆ†å‰²è¿‡å¤§çš„ç« èŠ‚
        ç­–ç•¥ï¼šæŒ‰æ®µè½ã€å¥å­è¿›è¡Œåˆ†å‰²
        """
        chunks = []
        # ä¼˜å…ˆæŒ‰åŒæ¢è¡Œç¬¦ï¼ˆæ®µè½ï¼‰åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        for p in paragraphs:
            p_tokens = self._count_tokens(p)
            current_chunk_tokens = self._count_tokens(current_chunk)
            
            if current_chunk_tokens + p_tokens <= self.target_chunk_size:
                current_chunk += "\n\n" + p if current_chunk else p
            else:
                if current_chunk:
                    chunks.append({"text": current_chunk, "metadata": metadata})
                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…é•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥æŒ‰å¥å­åˆ†å‰² (æ­¤å¤„ç®€åŒ–ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªå—)
                current_chunk = p
        
        if current_chunk:
            chunks.append({"text": current_chunk, "metadata": metadata})
            
        return chunks

    def _format_chunks_with_overlap(self, chunks: List[Dict]) -> List[Dict]:
        """
        ä¸ºåˆ†å—ç»“æœæ·»åŠ é‡å ï¼Œå¹¶ç”Ÿæˆæœ€ç»ˆæ ¼å¼
        """
        final_formatted_chunks = []
        
        for i, chunk in enumerate(chunks):
            # åœ¨å½“å‰å—çš„å¼€å¤´æ·»åŠ å‰ä¸€ä¸ªå—çš„ç»“å°¾éƒ¨åˆ†ä½œä¸ºé‡å 
            if i > 0 and self.chunk_overlap > 0:
                prev_chunk_text = chunks[i-1]['text']
                # ç¡®ä¿é‡å å†…å®¹æ¥è‡ªç›¸åŒçš„çˆ¶çº§ç« èŠ‚ï¼Œé¿å…ä¸ç›¸å…³çš„é‡å 
                if chunks[i-1]['metadata']['parent_titles'] == chunk['metadata']['parent_titles']:
                    overlap_text = self._get_overlap_text(prev_chunk_text)
                    chunk['text'] = overlap_text + "\n...\n" + chunk['text']

            # æ›´æ–°tokenè®¡æ•°
            chunk['metadata']['tokens'] = self._count_tokens(chunk['text'])
            final_formatted_chunks.append(chunk)
            
        return final_formatted_chunks

    def _get_overlap_text(self, text: str) -> str:
        """è·å–ç”¨äºé‡å çš„æ–‡æœ¬ç‰‡æ®µ"""
        tokens = self.encoding.encode(text, disallowed_special=())
        if len(tokens) <= self.chunk_overlap:
            return text
        
        overlap_tokens = tokens[-self.chunk_overlap:]
        return self.encoding.decode(overlap_tokens)


# --- æµ‹è¯•ä»£ç  ---
async def test_rag_indexer():
    import os
    print("ğŸ§ª å¼€å§‹æµ‹è¯• RAGIndexer (ä¼˜åŒ–ç‰ˆ)...")
    
    # è¯»å–çŸ¥è¯†åº“æ–‡ä»¶
    knowledge_file = "rag_prompt/ALL.txt"
    if not os.path.exists(knowledge_file):
        print(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {knowledge_file}")
        return
        
    with open(knowledge_file, 'r', encoding='utf-8') as f:
        content = f.read()
        
    # åˆ›å»ºç´¢å¼•å™¨å®ä¾‹
    indexer = RAGIndexer(chunk_size=400, chunk_overlap=50)
    
    # æ‰§è¡Œæ™ºèƒ½åˆ†å—
    chunks = indexer.smart_split(content, metadata={'source': knowledge_file})
    
    print(f"\nâœ… æ™ºèƒ½åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªå—ã€‚")
    
    # æ‰“å°ä¸€äº›ç¤ºä¾‹å—
    print("\n--- ç¤ºä¾‹å— ---")
    for i, chunk in enumerate(chunks):
        # ä»…æ‰“å°ä¸€äº›æœ‰ä»£è¡¨æ€§çš„å—
        if i < 3 or i > len(chunks) - 4:
            print(f"\nå— {i+1}:")
            print(f"  å…ƒæ•°æ®: {chunk['metadata']}")
            print(f"  å†…å®¹é¢„è§ˆ: {chunk['text'][:200].strip().replace(chr(10), ' ')}...")
        elif i == 3:
            print("\n...")

class RAGIndexerCog(commands.Cog):
    """RAGç´¢å¼•å™¨Cogï¼Œæä¾›æ–‡æ¡£æ™ºèƒ½åˆ†å—åŠŸèƒ½"""
    
    def __init__(self, bot):
        self.bot = bot
        self.indexer = RAGIndexer()
    
    @commands.command(name="test_indexer")
    async def test_indexer_command(self, ctx):
        """æµ‹è¯•RAGç´¢å¼•å™¨åŠŸèƒ½"""
        await ctx.send("RAGç´¢å¼•å™¨åŠŸèƒ½æ­£å¸¸è¿è¡Œ")

async def setup(bot):
    """è®¾ç½®å‡½æ•°ï¼Œç”¨äºåŠ è½½cog"""
    await bot.add_cog(RAGIndexerCog(bot))

if __name__ == "__main__":
    import asyncio
    asyncio.run(test_rag_indexer())